{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction from preprocessed data\n",
    "The following code is used to extract local features of given preprocessed graph relations. It is structured such as to work with a large amount of relations and standard notebook capacities. The extracted features represent specifications for a directed, multirelational graph or in a certain sense a weighted graph with a finite and discret number of weights.\n",
    "\n",
    "The input relations have to be preprocessed so that for each node all edges are gathered side by side. This means that all relations need to be listed twice in order to be able to gather them next to each node. This preprocessing allows to trade storage capacity for some computation speed. A unix script implementation for this is provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import subprocess\n",
    "import datetime\n",
    "import math\n",
    "import timeit\n",
    "\n",
    "def restart_line():\n",
    "    sys.stdout.write('\\r')\n",
    "    sys.stdout.flush()\n",
    "\n",
    "# defining input file\n",
    "filename = 'relations_red_ext.csv'\n",
    "path = \"/Users/jonasmuller/Local_Folder/Project NTDS/\"\n",
    "file = path + filename\n",
    "\n",
    "columns=[\"src\", \"dst\",\"relation\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure of the following code is to read the input *.csv* file in several chunks taking. The defined features are then computed locally in the chunk and appended to the feature DataFrame. An estimation of the remaining time is given periodically.\n",
    "\n",
    "The features have the following interpretation where \"#\" denotes the feature is calculated for each relation individually.\n",
    "* dtot_#: total degree\n",
    "* dout_#: outgoing degree\n",
    "* duni_#: number of unique neighbors\n",
    "* dnbi_#: number of bidirectional edges\n",
    "* n_bidir: total number of bidirectional edges\n",
    "\n",
    "Note that multiple relations between two nodes are possible and therefore the number of unique neighbors is in general not equal to the degrees. The difference between *dtot* and *duni* represents the number of multiple edges. In other words dtot counts the number of \"communications\" from a node while duni counts the number of \"recipients\".\n",
    "\n",
    "The number of bidirectional edges dnbi is a measure of how many interactions are actually answered at least once. This too, is in general different from the total degree and the unique neighbors. For example, in a social network a \"like\" from one node might not elicit a \"like\" from the recipient.\n",
    "\n",
    "Furthermore the n_bidir is generally different from the sum of the dnbi since a given interaction might cause an interaction of a different kind (relation). For example, the \"like\" of one node might induce a \"message\" from the recepeient as a response. Note also that n_bidir can in fact even be smaller than the sum of dnbi in the case when a certain edge is actually bidirectional in multiple relations which is counted as one bidirectional relation in n_bidir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select execution mode\n",
    "calc_stat = False\n",
    "debug = True\n",
    "save_features = False\n",
    "bidirectionality = True\n",
    "find_unique_neigh = True\n",
    "weights = False\n",
    "weight_KL = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Chunks: 430\n"
     ]
    }
   ],
   "source": [
    "# initialize shit\n",
    "chunksize = 4000000\n",
    "i_chunk = 0\n",
    "L = 1000000000\n",
    "l = 0\n",
    "\n",
    "carry = pd.DataFrame()\n",
    "df_buffer = pd.DataFrame()\n",
    "features = pd.DataFrame()\n",
    "rel_dist= pd.DataFrame(np.zeros(7),list(range(1,8)),columns=[\"count\"])\n",
    "t1 = time.time()\n",
    "told = t1\n",
    "n_chunks = np.ceil(1716494198/chunksize).astype(np.uint32)\n",
    "print(f\"Number of Chunks: {n_chunks:}\")\n",
    "t_chunk = np.zeros([n_chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_degrees(chunk):\n",
    "    # get degrees per relation\n",
    "    deg_rel = pd.DataFrame(chunk.groupby('src').relation.value_counts()).rename(columns={'relation': \"rel_count\"})\n",
    "    deg_rel = deg_rel.unstack(level=-1,fill_value=0)\n",
    "    deg_rel.columns = deg_rel.columns.droplevel(0)\n",
    "    del deg_rel.columns.name\n",
    "    col_names = deg_rel.columns\n",
    "    deg_rel_out = deg_rel[col_names[:7]]\n",
    "    deg_rel_in = deg_rel[col_names[7:]]\n",
    "    deg_rel_in.columns = np.mod(deg_rel_in.columns.to_numpy(),10)\n",
    "    deg_rel = deg_rel_out.add(deg_rel_in)\n",
    "    deg_rel = deg_rel.add_prefix('dtot_')\n",
    "    deg_rel_out = deg_rel_out.add_prefix('dout_')\n",
    "    deg_rel = deg_rel.join(deg_rel_out)\n",
    "    return deg_rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_unique_neighbors(chunk):\n",
    "    unique_neighbors = chunk_copy.groupby(['src','relation']).dst.nunique()\n",
    "    unique_neighbors = unique_neighbors.unstack(level=1,fill_value=0)\n",
    "    del unique_neighbors.columns.name\n",
    "    unique_neighbors = unique_neighbors.add_prefix('duni_')\n",
    "    return unique_neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_bidirectionality(chunk):\n",
    "    chunk_copy = chunk.copy()\n",
    "    chunk_copy[\"in\"] = (chunk_copy[\"relation\"] >= 10).astype(int)\n",
    "\n",
    "    # check each src-dst tuple to be in & out relation\n",
    "    nr_of_bidirect = chunk_copy.groupby(['src','dst'],squeeze=True).nunique()-1\n",
    "    nr_of_bidirect = nr_of_bidirect.drop(columns=['src','dst','relation'])\n",
    "    nr_of_bidirect = nr_of_bidirect.groupby(['src']).sum()['in']\n",
    "\n",
    "    # bidirectionality per relation\n",
    "    chunk_copy['relation'] = chunk_copy['relation'].mod(10)\n",
    "\n",
    "    bidirect = chunk_copy.groupby(['src','relation','dst']).nunique()-1\n",
    "    bidirect = bidirect.drop(columns=['src','dst','relation']).reset_index(level=-1,drop=True)\n",
    "    bidirect = bidirect.groupby(['src','relation']).sum()\n",
    "\n",
    "    bidirect = bidirect.unstack(level=1,fill_value=0)\n",
    "    bidirect.columns = bidirect.columns.droplevel(level=0)\n",
    "    del bidirect.columns.name\n",
    "    df_nbi = bidirect.add_prefix('dnbi_')\n",
    "    return nr_of_bidirect, df_nbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk Nr: 2; \t feat_shape: (37436, 29) \t tc: 9.64 sec \t te: 68.8 mm\n",
      "\n",
      "Execution time 0 min 39.26 sec\n",
      "Happy Ending!\n"
     ]
    }
   ],
   "source": [
    "# kick off the ordeal\n",
    "for chunk in pd.read_csv(file, chunksize=chunksize, sep = \"\\t\", names=columns):\n",
    "    \n",
    "    # slice chunk at specific src\n",
    "    chunk = carry.append(chunk)\n",
    "    chunk_copy = chunk.copy()\n",
    "    chunk_copy['relation'] = chunk_copy['relation'].mod(10)\n",
    "    \n",
    "    # remove last source in chunk to add it to the next one\n",
    "    last_src = chunk[\"src\"].iloc[-1]\n",
    "    carry = chunk.loc[chunk[\"src\"] == last_src]\n",
    "    chunk = chunk.loc[chunk[\"src\"] != last_src]\n",
    "    \n",
    "    deg_rel = get_degrees(chunk)\n",
    "    \n",
    "    # find number of unique neighbors per relation\n",
    "    if find_unique_neigh:\n",
    "        unique_neighbors = find_unique_neighbors(chunk)\n",
    "    \n",
    "    # check bidirectionality\n",
    "    if bidirectionality:\n",
    "        nr_of_bidirect, df_nbi = check_bidirectionality(chunk)\n",
    "        \n",
    "    # relation distribution count\n",
    "    if calc_stat:\n",
    "        values, counts = np.unique(chunk.loc[chunk[\"relation\"] < 10].relation.values, return_counts=True)\n",
    "        rel_dist = rel_dist.add(pd.Series(counts, index=values),axis='index')\n",
    "    \n",
    "    # append degrees\n",
    "    df_buffer = deg_rel\n",
    "    if find_unique_neigh:\n",
    "        df_buffer = df_buffer.join(unique_neighbors, on='src')\n",
    "    if bidirectionality:\n",
    "        df_buffer = df_buffer.join(df_nbi, on='src')\n",
    "        df_buffer['n_bidir'] = pd.Series(nr_of_bidirect)\n",
    "    if weights:\n",
    "        df_buffer['w_out'] = pd.Series(w_out)\n",
    "        df_buffer['w_in'] = pd.Series(w_in)\n",
    "    if weight_KL:\n",
    "        df_buffer['w_KL'] = pd.Series(w_KL)\n",
    "    \n",
    "    df_buffer = df_buffer.fillna(0)\n",
    "    \n",
    "    features = features.append(df_buffer.astype('uint32'))\n",
    "    \n",
    "    if save_bidirect_rel:\n",
    "        l2 = len(bidirect)\n",
    "        relations_bidirect[l:(l+l2),:] = bidirect.to_numpy()\n",
    "        if l2 >= 0:\n",
    "            l += l2+1\n",
    "    \n",
    "    # print status\n",
    "    s = (time.time()-told)\n",
    "    told = time.time()\n",
    "    t_chunk[i_chunk] = s\n",
    "    if i_chunk < 10:\n",
    "        t_est = t_chunk[i_chunk]*(n_chunks-i_chunk)/60\n",
    "    else:\n",
    "        t_est = np.mean(t_chunk[(i_chunk-10):i_chunk])*(n_chunks-i_chunk)/60\n",
    "        \n",
    "    # break for debugging purpose\n",
    "    if debug:\n",
    "        if i_chunk >= 5 or (time.time()-t1) > 30:\n",
    "            break\n",
    "\n",
    "    restart_line()\n",
    "    sys.stdout.write(f'Chunk Nr: {i_chunk}; \\t feat_shape: {features.shape} \\t tc: {s:.2f} sec \\t te: {t_est:.1f} m')\n",
    "    sys.stdout.flush()\n",
    "    i_chunk += 1\n",
    "\n",
    "if save_bidirect_rel:\n",
    "    relations_bidirect = relations_bidirect[0:(l-1),:]\n",
    "\n",
    "# print relevant key data\n",
    "m = math.floor((time.time()-t1)/60)\n",
    "s = (time.time()-t1)-m*60\n",
    "print(f'\\n\\nExecution time {m} min {s:.2f} sec')\n",
    "\n",
    "# statistics\n",
    "total_relations = np.sum(rel_dist.values)\n",
    "rel_dist = rel_dist.div(total_relations)\n",
    "with open(\"Output.txt\", \"w\") as text_file:\n",
    "    print(f\"Nr of total relations: {total_relations}\\n\", file=text_file)\n",
    "    print(\"Relations Distribution:\\n\", file=text_file)\n",
    "    print(rel_dist, file=text_file)\n",
    "\n",
    "# save features\n",
    "if save_features:\n",
    "    exportName = \"relations_features.csv\"\n",
    "    features.to_csv(exportName, sep = '\\t')\n",
    "    if save_bidirect_rel:\n",
    "        np.savetxt(\"relations_bidirectional.csv\", relations_bidirect, fmt='%i', delimiter=\"\\t\")\n",
    "    \n",
    "print(\"Happy Ending!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize time vector\n",
    "ax = plt.figure(1, figsize=(15, 1))\n",
    "plt.plot(t_chunk);\n",
    "ax = plt.figure(2, figsize=(15, 1))\n",
    "plt.hist(t_chunk, bins=100);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction from preprocessed data\n",
    "Relevant features are extracted directly from the relations data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import subprocess\n",
    "import datetime\n",
    "import math\n",
    "import timeit\n",
    "\n",
    "def restart_line():\n",
    "    sys.stdout.write('\\r')\n",
    "    sys.stdout.flush()\n",
    "\n",
    "filenames =['relations_extended.csv','relations_red_ext.csv']\n",
    "\n",
    "path = \"/Users/jonasmuller/Local_Folder/Project NTDS/\"\n",
    "file = path + filenames[1]\n",
    "\n",
    "# using dataset without timestamps\n",
    "reducedDataset = True\n",
    "if reducedDataset:\n",
    "    columns=[\"src\", \"dst\",\"relation\"]\n",
    "else:\n",
    "    columns=[\"day\", \"time_ms\", \"src\", \"dst\",\"relation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Chunks: 1717.0\n",
      "Chunk Nr: 1717; feature shape: (5321960, 4)\n",
      "Execution time 37 min 44.21 sec\n",
      "relations_extended_features.csv\n"
     ]
    }
   ],
   "source": [
    "chunksize = 1000000\n",
    "i = 0\n",
    "carry = pd.DataFrame()\n",
    "df_buffer = pd.DataFrame()\n",
    "features = pd.DataFrame()\n",
    "rel_dist = pd.DataFrame(np.zeros(7),list(range(1,8)),columns=[\"count\"])\n",
    "t1 = time.time()\n",
    "print(f\"Number of Chunks: {np.ceil(1716494198/chunksize):}\")\n",
    "for chunk in pd.read_csv(file, chunksize=chunksize, sep = \"\\t\", names=columns):\n",
    "    \n",
    "    # slice chunk at specific src\n",
    "    chunk = carry.append(chunk)\n",
    "    \n",
    "    # remove last source in chunk\n",
    "    last_src = np.unique(chunk[\"src\"].values)[-1]\n",
    "    carry = chunk.loc[chunk[\"src\"] == last_src]\n",
    "    \n",
    "    chunk = chunk.loc[chunk[\"src\"] != last_src]\n",
    "    sources_in_chunk = np.unique(chunk[\"src\"].values)\n",
    "    \n",
    "    # extract features        \n",
    "    degree_tot = chunk.groupby('src').dst.count()\n",
    "    degree_out = chunk.loc[chunk[\"relation\"] < 10].groupby('src').dst.count()\n",
    "    unique_neighbors = chunk.groupby('src').dst.nunique()\n",
    "    \n",
    "    chunk[\"relation\"] = ((chunk[\"relation\"] - 10) >= 0).astype(int)\n",
    "    bidirect = chunk.groupby(['src','dst']).max() - chunk.groupby(['src','dst']).min()\n",
    "\n",
    "    nr_of_bidirect = bidirect.groupby('src').relation.sum()\n",
    "    \n",
    "    # intersection between in and out relation nodes\n",
    "    \n",
    "    # weights acording to relation scarcity (then deg in deg out) / number of strong connections per node\n",
    "    \n",
    "    # relation distribution count\n",
    "    values, counts = np.unique(chunk.loc[chunk[\"relation\"] < 10].relation.values, return_counts=True)\n",
    "    rel_dist = rel_dist.add(pd.Series(counts, index=values),axis='index')\n",
    "    \n",
    "    # append degrees\n",
    "    df_buffer = df_buffer[0:0]\n",
    "    df_buffer['deg_tot'] = pd.Series(degree_tot)\n",
    "    df_buffer['deg_out'] = pd.Series(degree_out)\n",
    "    df_buffer['uni_neigh'] = pd.Series(unique_neighbors)\n",
    "    df_buffer['nr_of_bidirect'] = pd.Series(nr_of_bidirect)\n",
    "    \n",
    "    df_buffer = df_buffer.fillna(0)\n",
    "    \n",
    "    features = features.append(df_buffer)\n",
    "    \n",
    "    # break for debugging\n",
    "    i = i+1\n",
    "    if i < 1:\n",
    "        break\n",
    "        \n",
    "    restart_line()\n",
    "    sys.stdout.write(f'Chunk Nr: {i}; feature shape: {features.shape}')\n",
    "    sys.stdout.flush()\n",
    "\n",
    "# print relevant key data\n",
    "m = math.floor((time.time()-t1)/60)\n",
    "s = (time.time()-t1)-m*60\n",
    "print(f'\\nExecution time {m} min {s:.2f} sec')\n",
    "\n",
    "# statistics\n",
    "total_relations = np.sum(rel_dist.values)\n",
    "rel_dist = rel_dist.div(total_relations)\n",
    "with open(\"Output.txt\", \"w\") as text_file:\n",
    "    print(f\"Nr of total relations: {total_relations}\\n\", file=text_file)\n",
    "    print(\"Relations Distribution:\\n\", file=text_file)\n",
    "    print(rel_dist, file=text_file)\n",
    "\n",
    "# save features\n",
    "if True:\n",
    "    exportName = filenames[0][:-4] + \"_features.csv\"\n",
    "    print(exportName)\n",
    "    features.to_csv(exportName, sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relations_extended_features.csv\n"
     ]
    }
   ],
   "source": [
    "# save features\n",
    "exportName = filenames[0][:-4] + \"_features.csv\"\n",
    "print(exportName)\n",
    "features.to_csv(exportName, sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize\n",
    "ax = plt.figure(1, figsize=(15, 3))\n",
    "plt.title(\"DegreeOut histogram\")\n",
    "plt.hist(dout.values, bins=200);\n",
    "plt.ylabel('Number of Nodes')\n",
    "plt.xlabel('Degree Out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method DataFrame.memory_usage of       deg_tot  deg_out  uni_neigh\n",
      "src                              \n",
      "1       275.0     90.0      166.0\n",
      "2       294.0     59.0      189.0\n",
      "3        57.0     34.0       34.0\n",
      "4        36.0     13.0       21.0\n",
      "5        38.0     16.0       23.0\n",
      "...       ...      ...        ...\n",
      "3322      NaN      NaN        NaN\n",
      "3323      NaN      NaN        NaN\n",
      "3324      NaN      NaN        NaN\n",
      "3325      NaN      NaN        NaN\n",
      "3326      NaN      NaN        NaN\n",
      "\n",
      "[632145 rows x 3 columns]>\n"
     ]
    }
   ],
   "source": [
    "print(features.memory_usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
